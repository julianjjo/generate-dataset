# Dataset Generator con Ollama

Generador de datasets masivos para entrenamiento de modelos de lenguaje usando Ollama. Capaz de generar hasta 100 millones de ejemplos con diferentes tipos de contenido de alta calidad.

## üöÄ Caracter√≠sticas

- **Generaci√≥n masiva**: Soporte para datasets de hasta 100M de ejemplos
- **M√∫ltiples tipos de contenido**: Cuentos, instrucciones, c√≥digo, art√≠culos, di√°logos y ensayos
- **Formato optimizado**: Compatible con `tokenize_function` est√°ndar
- **Procesamiento as√≠ncrono**: Generaci√≥n eficiente con control de concurrencia
- **Sistema de checkpoints**: Recuperaci√≥n autom√°tica en caso de interrupciones
- **Consolidaci√≥n autom√°tica**: Combina m√∫ltiples archivos en un dataset final

## üìã Requisitos

- Python 3.8+
- Ollama instalado y ejecut√°ndose
- Al menos un modelo de Ollama descargado (ej: `llama3.1`, `codellama`)

## üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio**:
   ```bash
   git clone <repository-url>
   cd generate-dataset
   ```

2. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Instalar y configurar Ollama**:
   ```bash
   # Descargar Ollama desde https://ollama.ai
   
   # Descargar un modelo (ejemplo)
   ollama pull llama3.1
   ollama pull codellama
   ```

4. **Verificar instalaci√≥n**:
   ```bash
   ollama list  # Debe mostrar los modelos descargados
   ollama serve # Iniciar servidor (puerto 11434 por defecto)
   ```

## üéØ Uso B√°sico

### Generaci√≥n Simple
```bash
# Generar 1000 ejemplos con configuraci√≥n por defecto
python main.py --size 1000

# Generar dataset peque√±o para pruebas
python main.py --size 100 --batch-size 10 --output prueba_dataset
```

### Configuraci√≥n Avanzada
```bash
# Dataset masivo con modelo espec√≠fico
python main.py --size 10000000 --model codellama --batch-size 200 --concurrent 30

# Usar servidor Ollama remoto
python main.py --ollama-url http://192.168.1.100:11434 --model llama3.1 --size 50000
```

### Solo Consolidaci√≥n
```bash
# Consolidar archivos existentes sin generar nuevos
python main.py --consolidate-only --output mi_dataset
```

## üìä Tipos de Dataset Generados

El generador crea 6 tipos diferentes de contenido:

| Tipo | Descripci√≥n | Tama√±o t√≠pico |
|------|-------------|---------------|
| **Cuentos** | Narrativas completas con inicio, desarrollo y final | 300-500 palabras |
| **Instrucciones** | Gu√≠as paso a paso educativas y t√©cnicas | 200+ palabras |
| **Di√°logos** | Conversaciones naturales con contexto | 8-10 intercambios |
| **Art√≠culos** | Textos informativos estructurados | 400-600 palabras |
| **C√≥digo** | Programas completos con comentarios | Funcional y documentado |
| **Ensayos** | Textos reflexivos y acad√©micos | 400-500 palabras |

## üîß Par√°metros de Configuraci√≥n

### Argumentos de l√≠nea de comandos

```bash
python main.py [opciones]
```

| Par√°metro | Descripci√≥n | Valor por defecto |
|-----------|-------------|-------------------|
| `--size` | N√∫mero total de ejemplos a generar | 100,000,000 |
| `--batch-size` | Ejemplos por lote | 100 |
| `--concurrent` | Tareas concurrentes m√°ximas | 20 |
| `--output` | Directorio de salida | "generated_dataset" |
| `--ollama-url` | URL del servidor Ollama | "http://localhost:11434" |
| `--model` | Modelo de Ollama a usar | "llama3.1" |
| `--consolidate-only` | Solo consolidar archivos existentes | False |

### Ejemplos de Uso por Escenario

#### Dataset para Fine-tuning General
```bash
python main.py --size 100000 --model llama3.1 --batch-size 50 --output general_dataset
```

#### Dataset de C√≥digo
```bash
python main.py --size 50000 --model codellama --batch-size 25 --output code_dataset
```

#### Dataset Masivo (Producci√≥n)
```bash
python main.py --size 50000000 --batch-size 500 --concurrent 50 --output production_dataset
```

## üìÅ Estructura de Salida

```
mi_dataset/
‚îú‚îÄ‚îÄ batch_000001.jsonl    # Lotes individuales
‚îú‚îÄ‚îÄ batch_000002.jsonl
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ checkpoint.json       # Progreso guardado
‚îî‚îÄ‚îÄ complete_dataset.jsonl # Dataset consolidado (opcional)
```

### Formato de Datos

Cada l√≠nea en los archivos `.jsonl` tiene el formato:

```json
{"text": "Contenido completo del ejemplo aqu√≠..."}
```

Este formato es **directamente compatible** con la funci√≥n `tokenize_function` est√°ndar que busca el campo `text`.

## üîÑ Sistema de Checkpoints

El generador incluye un sistema robusto de checkpoints:

- **Guardado autom√°tico**: Cada 10,000 ejemplos generados
- **Recuperaci√≥n autom√°tica**: Reanuda desde el √∫ltimo checkpoint
- **Informaci√≥n de progreso**: Tracking detallado del avance

Ejemplo de checkpoint:
```json
{
  "generated_count": 50000,
  "timestamp": 1692123456.789,
  "progress": 50.0
}
```

## üìà Rendimiento y Optimizaci√≥n

### Recomendaciones por Tama√±o de Dataset

| Tama√±o del Dataset | Batch Size | Concurrent | Tiempo Estimado* |
|-------------------|------------|------------|------------------|
| 1K - 10K | 10-25 | 5-10 | 10-30 min |
| 10K - 100K | 25-100 | 10-20 | 1-5 horas |
| 100K - 1M | 100-200 | 20-30 | 5-20 horas |
| 1M+ | 200-500 | 30-50 | 20+ horas |

*Los tiempos dependen del modelo, hardware y configuraci√≥n de Ollama.

### Consejos de Optimizaci√≥n

1. **Ajustar concurrencia**: M√°s concurrent tasks = mayor uso de memoria
2. **Batch size √≥ptimo**: Balance entre memoria y eficiencia de red
3. **Modelo adecuado**: Modelos m√°s peque√±os = generaci√≥n m√°s r√°pida
4. **Recursos del sistema**: Monitor CPU y memoria durante generaci√≥n

## üêõ Soluci√≥n de Problemas

### Errores Comunes

#### "Cannot connect to host localhost:11434"
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# En otra terminal, probar conexi√≥n
curl http://localhost:11434/api/tags
```

#### "Model not found"
```bash
# Listar modelos disponibles
ollama list

# Descargar el modelo necesario
ollama pull llama3.1
```

#### "Out of memory"
- Reducir `--concurrent` y `--batch-size`
- Usar un modelo m√°s peque√±o
- Cerrar otras aplicaciones que consuman memoria

#### Generaci√≥n muy lenta
- Verificar recursos del sistema (CPU, memoria)
- Usar un modelo m√°s r√°pido (ej: `llama3.1` vs `llama3.1:70b`)
- Ajustar par√°metros de concurrencia

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Add nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles.

## üîó Enlaces √ötiles

- [Ollama Official Website](https://ollama.ai)
- [Ollama Models Library](https://ollama.ai/library)
- [Python AsyncIO Documentation](https://docs.python.org/3/library/asyncio.html)

---

‚ö° **Tip**: Para datasets muy grandes, considera ejecutar el generador en un servidor dedicado con buena conectividad y recursos computacionales adecuados.
