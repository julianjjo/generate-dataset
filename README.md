# Dataset Generator con Ollama

Generador de datasets masivos para entrenamiento de modelos de lenguaje usando Ollama. Capaz de generar hasta 100 millones de ejemplos con diferentes tipos de contenido de alta calidad.

## üöÄ Caracter√≠sticas

- **Generaci√≥n masiva**: Soporte para datasets de hasta 100M de ejemplos
- **Soporte multiidioma**: Espa√±ol, ingl√©s o contenido mixto
- **M√∫ltiples tipos de contenido**: Cuentos, instrucciones, c√≥digo, art√≠culos, di√°logos y ensayos
- **Formato optimizado**: Compatible con `tokenize_function` est√°ndar
- **Procesamiento as√≠ncrono**: Generaci√≥n eficiente con control de concurrencia
- **Sistema de checkpoints**: Recuperaci√≥n autom√°tica en caso de interrupciones
- **Progreso en tiempo real**: Logs detallados y barra de progreso actualizada
- **Consolidaci√≥n autom√°tica**: Combina m√∫ltiples archivos en un dataset final

## üìã Requisitos

- Python 3.8+
- Ollama instalado y ejecut√°ndose
- Al menos un modelo de Ollama descargado (ej: `llama3.1`, `codellama`)

## üõ†Ô∏è Instalaci√≥n

1. **Clonar el repositorio**:
   ```bash
   git clone <repository-url>
   cd generate-dataset
   ```

2. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Instalar y configurar Ollama**:
   ```bash
   # Descargar Ollama desde https://ollama.ai
   
   # Descargar un modelo (ejemplo)
   ollama pull llama3.1
   ollama pull codellama
   ```

4. **Verificar instalaci√≥n**:
   ```bash
   ollama list  # Debe mostrar los modelos descargados
   ollama serve # Iniciar servidor (puerto 11434 por defecto)
   ```

## üéØ Uso B√°sico

### Generaci√≥n Simple
```bash
# Generar 1000 ejemplos en espa√±ol (por defecto)
python main.py --size 1000

# Generar dataset peque√±o en ingl√©s
python main.py --size 100 --batch-size 10 --language en --output english_dataset

# Generar dataset mixto (espa√±ol + ingl√©s)
python main.py --size 500 --language mixed --output multilingual_dataset
```

### Configuraci√≥n Avanzada
```bash
# Dataset masivo en ingl√©s con modelo espec√≠fico
python main.py --size 10000000 --model codellama --batch-size 200 --concurrent 30 --language en

# Dataset mixto usando servidor Ollama remoto
python main.py --ollama-url http://192.168.1.100:11434 --model llama3.1 --size 50000 --language mixed

# Dataset especializado en c√≥digo con CodeLlama
python main.py --model codellama --size 25000 --language en --output code_dataset
```

### Solo Consolidaci√≥n
```bash
# Consolidar archivos existentes sin generar nuevos
python main.py --consolidate-only --output mi_dataset
```

## üåç Soporte Multiidioma

El generador soporta tres modos de idioma:

| Modo | Descripci√≥n | Uso |
|------|-------------|-----|
| **Espa√±ol (`es`)** | Todo el contenido en espa√±ol | `--language es` (por defecto) |
| **Ingl√©s (`en`)** | Todo el contenido en ingl√©s | `--language en` |
| **Mixto (`mixed`)** | Alterna aleatoriamente entre espa√±ol e ingl√©s | `--language mixed` |

### Ejemplos de Uso por Idioma

```bash
# Dataset completamente en espa√±ol
python main.py --size 10000 --language es --output spanish_dataset

# Dataset completamente en ingl√©s  
python main.py --size 10000 --language en --output english_dataset

# Dataset mixto (ideal para modelos multiling√ºes)
python main.py --size 10000 --language mixed --output multilingual_dataset
```

## üìä Tipos de Dataset Generados

El generador crea 6 tipos diferentes de contenido en ambos idiomas:

| Tipo | Descripci√≥n | Tama√±o t√≠pico |
|------|-------------|---------------|
| **Cuentos** | Narrativas completas con inicio, desarrollo y final | 300-500 palabras |
| **Instrucciones** | Gu√≠as paso a paso educativas y t√©cnicas | 200+ palabras |
| **Di√°logos** | Conversaciones naturales con contexto | 8-10 intercambios |
| **Art√≠culos** | Textos informativos estructurados | 400-600 palabras |
| **C√≥digo** | Programas completos con comentarios | Funcional y documentado |
| **Ensayos** | Textos reflexivos y acad√©micos | 400-500 palabras |

## üîß Par√°metros de Configuraci√≥n

### Argumentos de l√≠nea de comandos

```bash
python main.py [opciones]
```

| Par√°metro | Descripci√≥n | Valor por defecto |
|-----------|-------------|-------------------|
| `--size` | N√∫mero total de ejemplos a generar | 100,000,000 |
| `--batch-size` | Ejemplos por lote | 100 |
| `--concurrent` | Tareas concurrentes m√°ximas | 20 |
| `--output` | Directorio de salida | "generated_dataset" |
| `--ollama-url` | URL del servidor Ollama | "http://localhost:11434" |
| `--model` | Modelo de Ollama a usar | "llama3.1" |
| `--language` | Idioma del dataset | "es" |
| `--consolidate-only` | Solo consolidar archivos existentes | False |

### Opciones de Idioma

| Valor | Descripci√≥n |
|-------|-------------|
| `es` | Genera todo el contenido en espa√±ol |
| `en` | Genera todo el contenido en ingl√©s |
| `mixed` | Alterna aleatoriamente entre espa√±ol e ingl√©s por ejemplo |

### Ejemplos de Uso por Escenario

#### Dataset para Fine-tuning General en Espa√±ol
```bash
python main.py --size 100000 --model llama3.1 --batch-size 50 --language es --output spanish_general
```

#### Dataset de C√≥digo en Ingl√©s
```bash
python main.py --size 50000 --model codellama --batch-size 25 --language en --output english_code
```

#### Dataset Multiling√ºe Masivo (Producci√≥n)
```bash
python main.py --size 50000000 --batch-size 500 --concurrent 50 --language mixed --output multilingual_production
```

#### Dataset Especializado por Idioma
```bash
# Instrucciones t√©cnicas en ingl√©s
python main.py --size 25000 --model llama3.1 --language en --output tech_instructions_en

# Contenido creativo en espa√±ol
python main.py --size 25000 --model llama3.1 --language es --output creative_content_es
```

## üìÅ Estructura de Salida

```
mi_dataset/
‚îú‚îÄ‚îÄ batch_000001.jsonl    # Lotes individuales
‚îú‚îÄ‚îÄ batch_000002.jsonl
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ checkpoint.json       # Progreso guardado
‚îî‚îÄ‚îÄ complete_dataset.jsonl # Dataset consolidado (opcional)
```

### Formato de Datos

Cada l√≠nea en los archivos `.jsonl` tiene el formato:

```json
{"text": "Contenido completo del ejemplo aqu√≠..."}
```

Este formato es **directamente compatible** con la funci√≥n `tokenize_function` est√°ndar que busca el campo `text`.

## üîÑ Sistema de Checkpoints y Progreso

El generador incluye un sistema robusto de checkpoints y monitoreo en tiempo real:

### Checkpoints Autom√°ticos
- **Guardado autom√°tico**: Cada 10,000 ejemplos generados
- **Recuperaci√≥n autom√°tica**: Reanuda desde el √∫ltimo checkpoint
- **Informaci√≥n de progreso**: Tracking detallado del avance

### Monitoreo en Tiempo Real
- **Logs detallados**: Informaci√≥n de cada lote procesado
- **Barra de progreso**: Actualizaci√≥n visual continua
- **Contadores din√°micos**: Ejemplos generados y porcentaje completado
- **Indicadores visuales**: Emojis para f√°cil identificaci√≥n (‚úì, üíæ)

### Ejemplo de Salida de Progreso
```
2025-08-23 23:18:19,489 - INFO - Iniciando generaci√≥n de dataset: 10,000 ejemplos
2025-08-23 23:18:21,279 - INFO - Conexi√≥n con Ollama establecida
2025-08-23 23:18:22,156 - INFO - Procesando lote 1/100
2025-08-23 23:18:25,789 - INFO - ‚úì Guardado lote 1: 100 elementos | Total: 100
2025-08-23 23:18:26,234 - INFO - Lote 1 completado: 100 ejemplos generados

Generando dataset: 15%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                     | 15/100 lotes [ejemplos: 1,500, progreso: 15.0%]

2025-08-23 23:25:34,123 - INFO - üíæ Checkpoint guardado: 10,000 elementos (100.0%)
```

### Formato de Checkpoint
```json
{
  "generated_count": 50000,
  "timestamp": 1692123456.789,
  "progress": 50.0
}
```

## üìà Rendimiento y Optimizaci√≥n

### Recomendaciones por Tama√±o de Dataset

| Tama√±o del Dataset | Batch Size | Concurrent | Tiempo Estimado* |
|-------------------|------------|------------|------------------|
| 1K - 10K | 10-25 | 5-10 | 10-30 min |
| 10K - 100K | 25-100 | 10-20 | 1-5 horas |
| 100K - 1M | 100-200 | 20-30 | 5-20 horas |
| 1M+ | 200-500 | 30-50 | 20+ horas |

*Los tiempos dependen del modelo, hardware y configuraci√≥n de Ollama.

### Consejos de Optimizaci√≥n

1. **Ajustar concurrencia**: M√°s concurrent tasks = mayor uso de memoria
2. **Batch size √≥ptimo**: Balance entre memoria y eficiencia de red
3. **Modelo adecuado**: Modelos m√°s peque√±os = generaci√≥n m√°s r√°pida
4. **Recursos del sistema**: Monitor CPU y memoria durante generaci√≥n

## üêõ Soluci√≥n de Problemas

### Errores Comunes

#### "Cannot connect to host localhost:11434"
```bash
# Verificar que Ollama est√© ejecut√°ndose
ollama serve

# En otra terminal, probar conexi√≥n
curl http://localhost:11434/api/tags
```

#### "Model not found"
```bash
# Listar modelos disponibles
ollama list

# Descargar el modelo necesario
ollama pull llama3.1
```

#### "Out of memory"
- Reducir `--concurrent` y `--batch-size`
- Usar un modelo m√°s peque√±o
- Cerrar otras aplicaciones que consuman memoria

#### Generaci√≥n muy lenta
- Verificar recursos del sistema (CPU, memoria)
- Usar un modelo m√°s r√°pido (ej: `llama3.1` vs `llama3.1:70b`)
- Ajustar par√°metros de concurrencia

#### Problemas con idiomas espec√≠ficos
- **Contenido en idioma incorrecto**: Verificar el par√°metro `--language`
- **Mezcla inconsistente**: En modo `mixed`, la alternancia es aleatoria por dise√±o
- **Modelos especializados**: Algunos modelos funcionan mejor con idiomas espec√≠ficos:
  - `llama3.1`: Excelente para espa√±ol e ingl√©s
  - `codellama`: Mejor para c√≥digo en ingl√©s
  - `mistral`: Bueno para contenido multiling√ºe

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Add nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver `LICENSE` para m√°s detalles.

## üîó Enlaces √ötiles

- [Ollama Official Website](https://ollama.ai)
- [Ollama Models Library](https://ollama.ai/library)
- [Python AsyncIO Documentation](https://docs.python.org/3/library/asyncio.html)

---

## üí° Tips y Mejores Pr√°cticas

### Para Datasets Multiling√ºes
- **Modo mixto**: Ideal para entrenar modelos que necesiten responder en ambos idiomas
- **Datasets separados**: Para fine-tuning espec√≠fico por idioma, genera datasets individuales
- **Verificaci√≥n de calidad**: Revisa algunos ejemplos para asegurar la calidad del idioma

### Para Datasets Masivos
- **Servidores dedicados**: Para datasets muy grandes, usa un servidor con buena conectividad
- **Monitoreo continuo**: Las mejoras de progreso te permiten monitorear generaciones largas
- **Checkpoints**: Los checkpoints autom√°ticos permiten reanudar generaciones interrumpidas

### Para Rendimiento √ìptimo
- **Concurrencia balanceada**: M√°s concurrent tasks = mayor memoria, pero tambi√©n mayor velocidad
- **Batch size apropiado**: Lotes m√°s grandes son m√°s eficientes pero consumen m√°s memoria
- **Modelo adecuado**: Elige el modelo seg√∫n el tipo de contenido que necesites

‚ö° **Recomendaci√≥n**: Para datasets de producci√≥n, inicia con una prueba peque√±a usando `--size 1000` para verificar calidad y rendimiento antes de generar el dataset completo.
